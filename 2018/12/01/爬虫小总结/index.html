<!DOCTYPE html>
<html lang=>
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="description" content="">
  <meta name="keywords" content="">
  
    <link rel="icon" href="/favicon.ico">
  
    
  <title>爬虫小总结 | tomxxkl的个人博客</title>
  <link rel="stylesheet" href="/style.css">
  <link rel="stylesheet" href="/lib/jquery.fancybox.min.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
  <header>
  <div class="header-container">
    <a class='logo' href="/">
      <span>tomxxkl的个人博客</span>
    </a>
    <ul class="right-header">
      
        <li class="nav-item">
          
            <a href="/" class="item-link">首页</a>
          
        </li>
      
        <li class="nav-item">
          
            <a href="/about" class="item-link">关于</a>
          
        </li>
      
        <li class="nav-item">
          
            <a href="/archives" class="item-link">归档</a>
          
        </li>
      
        <li class="nav-item">
          
            <a href="/tags" class="item-link">标签</a>
          
        </li>
      
    </ul>
  </div>
</header>

  <main id='post'>
  <div class="content">
    <article>
        <section class="content markdown-body">
          <h1>爬虫小总结</h1>
          <div class='post-meta'>
            <i class="fa fa-calendar" aria-hidden="true"></i> <time>2018/12/01</time>
            
            
          </div>
          <h3 id="目前遇到的数据加载方式"><a href="#目前遇到的数据加载方式" class="headerlink" title="目前遇到的数据加载方式"></a>目前遇到的数据加载方式</h3><h4 id="1-数据直接在网页上"><a href="#1-数据直接在网页上" class="headerlink" title="1.数据直接在网页上"></a>1.数据直接在网页上</h4><p>数据直接在网页上，那么获取就相当简单，通过requests库和lxml获取bs4或者正则表达式就能直接获取并解析</p>
<p>例子:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""__author__=wuliang"""</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> maoyan_db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page</span><span class="params">(url)</span>:</span></span><br><span class="line">	headers = &#123;</span><br><span class="line">		<span class="string">'User-Agent'</span>: <span class="string">'User-Agent:Mozilla/5.0 (Windows NT 10.0; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0'</span></span><br><span class="line">	&#125;</span><br><span class="line">	response = requests.get(url, headers=headers)</span><br><span class="line">	<span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">		<span class="keyword">return</span> response.content.decode(<span class="string">'utf8'</span>)</span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(html)</span>:</span></span><br><span class="line">	result_list = []</span><br><span class="line">	<span class="comment"># 主演</span></span><br><span class="line">	pattern = re.compile(<span class="string">r'&lt;p class="star"&gt;(.*?)&lt;/p&gt;'</span>, re.S)</span><br><span class="line">	stars = re.findall(pattern, html)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 标题</span></span><br><span class="line">	pattern = re.compile(<span class="string">r'movieId.*?&gt;.*?&lt;img.*?&lt;img.*?alt="(.*?)" class.*?'</span>, re.S)</span><br><span class="line">	titles = re.findall(pattern, html)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 上映时间</span></span><br><span class="line">	pattern = re.compile(<span class="string">r'&lt;p class="releasetime"&gt;(.*?)&lt;/p&gt;'</span>, re.S)</span><br><span class="line">	times = re.findall(pattern,html)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 图片链接</span></span><br><span class="line">	pattern = re.compile(<span class="string">r'movieId.*?&gt;.*?&lt;img.*?&lt;img.*?src=(.*?) alt.*?'</span>,re.S)</span><br><span class="line">	links = re.findall(pattern, html)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 排名</span></span><br><span class="line">	pattern = re.compile(<span class="string">r'&lt;dd&gt;.*?board-index.*?&gt;(.*?)&lt;/i&gt;'</span>,re.S)</span><br><span class="line">	ranks = re.findall(pattern, html)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 评分</span></span><br><span class="line">	pattern = re.compile(<span class="string">r'&lt;p class="score".*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;'</span>,re.S)</span><br><span class="line">	scores = re.findall(pattern, html)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 获取数据连接</span></span><br><span class="line">	conn = maoyan_db.get_db_connection()</span><br><span class="line">	cursor = maoyan_db.get_cursor(conn)</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(len(ranks)):</span><br><span class="line">		result_dict = &#123;</span><br><span class="line">			<span class="string">'star'</span>: stars[i].strip(),</span><br><span class="line">			<span class="string">'title'</span>: titles[i].strip(),</span><br><span class="line">			<span class="string">'time'</span>: times[i].strip(),</span><br><span class="line">			<span class="string">'link'</span>: links[i].strip(),</span><br><span class="line">			<span class="string">'rank'</span>: ranks[i].strip(),</span><br><span class="line">			<span class="string">'score'</span>: <span class="string">''</span>.join(scores[i]),</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment"># 插入数据库</span></span><br><span class="line">		maoyan_db.insert_record(conn,cursor,result_dict)</span><br><span class="line">		result_list.append(result_dict)</span><br><span class="line">	<span class="comment"># 关闭数据库连接</span></span><br><span class="line">	maoyan_db.close_conn(conn)</span><br><span class="line">	<span class="keyword">return</span> result_list</span><br><span class="line"></span><br><span class="line">	<span class="comment"># for item in items:</span></span><br><span class="line">	<span class="comment"># 	print(''.join(item))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_pages</span><span class="params">(url)</span>:</span></span><br><span class="line">	result_all_list = []</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">100</span>, <span class="number">10</span>):</span><br><span class="line">		html = get_page(url+<span class="string">'?offset='</span> + str(i))</span><br><span class="line">		result_all_list.extend(parse_page(html))</span><br><span class="line">	<span class="keyword">return</span> result_all_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_img</span><span class="params">(result_all_list)</span>:</span></span><br><span class="line">	<span class="keyword">for</span> item <span class="keyword">in</span> result_all_list:</span><br><span class="line">		url = item[<span class="string">'link'</span>]</span><br><span class="line">		response = requests.get(url.replace(<span class="string">'\"'</span>,<span class="string">''</span>))</span><br><span class="line">		filename = url.split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'@'</span>)[<span class="number">0</span>]</span><br><span class="line">		<span class="keyword">with</span> open(<span class="string">'./images/'</span>+filename,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">			f.write(response.content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_json</span><span class="params">(result_all_list)</span>:</span></span><br><span class="line">	json_text = json.dumps(result_all_list, ensure_ascii=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">with</span> open(<span class="string">'./maoyan.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">		f.write(json_text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">	result_all_list = get_all_pages(<span class="string">'http://maoyan.com/board/4'</span>)</span><br><span class="line">	<span class="comment"># save_img(result_all_list)</span></span><br><span class="line">	save_json(result_all_list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">	main()</span><br></pre></td></tr></table></figure>
<h4 id="2-数据通过异步加载的方式返回的"><a href="#2-数据通过异步加载的方式返回的" class="headerlink" title="2.数据通过异步加载的方式返回的"></a>2.数据通过异步加载的方式返回的</h4><p>数据如果是通过异步加载的方式返回的，那么就需要先去找到异步请求的网站，通过这个链接去获取相应的数据，这种数据获取方式最难的是找到异步请求的网址。数据获取部分就很简单了</p>
<p>例子：爬取蘑菇街</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""__author__=wuliang"""</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mogujie <span class="keyword">import</span> sqlalchemy_helper</span><br><span class="line"></span><br><span class="line">headers_list = [</span><br><span class="line">	<span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60'</span>,</span><br><span class="line">	<span class="string">'Opera/8.0 (Windows NT 5.1; U; en)'</span>,</span><br><span class="line">	<span class="string">'Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50'</span>,</span><br><span class="line">	<span class="string">'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50'</span>,</span><br><span class="line">	<span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0'</span>,</span><br><span class="line">	<span class="string">'Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10'</span>,</span><br><span class="line">	<span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36'</span>,</span><br><span class="line">	<span class="string">'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11'</span>,</span><br><span class="line">	<span class="string">'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16'</span>,</span><br><span class="line">	<span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36'</span>,</span><br><span class="line">	<span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko'</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page</span><span class="params">(url)</span>:</span></span><br><span class="line">	headers = &#123;</span><br><span class="line">		<span class="string">'User-Agent'</span>: random.choice(headers_list),</span><br><span class="line">	&#125;</span><br><span class="line">	response = requests.get(url, headers=headers)</span><br><span class="line">	<span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">		<span class="keyword">return</span> response.content.decode(<span class="string">'utf8'</span>)</span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(html)</span>:</span></span><br><span class="line">	start = html.index(<span class="string">'('</span>)</span><br><span class="line">	html = html[start+<span class="number">1</span>:<span class="number">-2</span>]</span><br><span class="line">	result_dict = json.loads(html)</span><br><span class="line">	results_list = []</span><br><span class="line">	results = result_dict[<span class="string">'result'</span>][<span class="string">'wall'</span>][<span class="string">'docs'</span>]</span><br><span class="line">	flag = result_dict[<span class="string">'result'</span>][<span class="string">'wall'</span>][<span class="string">'isEnd'</span>]</span><br><span class="line">	<span class="keyword">for</span> item <span class="keyword">in</span> results:</span><br><span class="line">		t_dic = &#123;</span><br><span class="line">			<span class="string">"tradeItemId"</span>: item.get(<span class="string">'tradeItemId'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"img"</span>: item.get(<span class="string">'img'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"itemType"</span>: item.get(<span class="string">'itemType'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"clientUrl"</span>: item.get(<span class="string">'clientUrl'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"link"</span>: item.get(<span class="string">'link'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"itemMarks"</span>: item.get(<span class="string">'itemMarks'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"acm"</span>: item.get(<span class="string">'acm'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"price_taglist"</span>: item.get(<span class="string">'price_taglist'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"title"</span>: item.get(<span class="string">'title'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"type"</span>: item.get(<span class="string">'type'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"orgPrice"</span>: item.get(<span class="string">'orgPrice'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"hasSimilarity"</span>: item.get(<span class="string">'hasSimilarity'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"lefttop_taglist"</span>: item.get(<span class="string">'lefttop_taglist'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"cfav"</span>: item.get(<span class="string">'cfav'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"price"</span>: item.get(<span class="string">'price'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"leftbottom_taglist"</span>: item.get(<span class="string">'leftbottom_taglist'</span>,<span class="string">''</span>),</span><br><span class="line">			<span class="string">"similarityUrl"</span>: item.get(<span class="string">'similarityUrl'</span>,<span class="string">''</span>)</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		results_list.append(t_dic)</span><br><span class="line">	<span class="keyword">return</span> (results_list,flag)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_as_json</span><span class="params">(results_list)</span>:</span></span><br><span class="line">	content = json.dumps(results_list,ensure_ascii=<span class="keyword">False</span>)</span><br><span class="line">	<span class="keyword">with</span> open(<span class="string">'./mogujies.txt'</span>,<span class="string">'a'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">		f.write(content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">	result = []</span><br><span class="line">	i = <span class="number">0</span></span><br><span class="line">	flag = <span class="keyword">False</span></span><br><span class="line">	<span class="keyword">while</span> <span class="keyword">not</span> flag:</span><br><span class="line">		i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">		page = <span class="string">'&amp;page='</span>+str(i)</span><br><span class="line">		url = <span class="string">'https://list.mogujie.com/search?callback=jQuery21103792090581266637_1543377039647&amp;_version=8193&amp;ratio=3%3A4&amp;cKey=43&amp;sort=pop'</span>+page</span><br><span class="line">		url_end = <span class="string">'&amp;q=%25E5%25A5%25B3%25E8%25A3%2585&amp;minPrice=&amp;maxPrice=&amp;ppath=&amp;cpc_offset=&amp;ptp=1.5y18ub.0.0.n5awCJ7r&amp;_=1543377039648'</span></span><br><span class="line">		url += url_end</span><br><span class="line">		html = get_page(url)</span><br><span class="line">		<span class="keyword">if</span> <span class="string">'('</span> <span class="keyword">not</span> <span class="keyword">in</span> html:</span><br><span class="line">			print(<span class="string">'-----in error-----'</span>)</span><br><span class="line">			time.sleep(<span class="number">1</span>)</span><br><span class="line">			i -= <span class="number">1</span></span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		tem = []</span><br><span class="line">		tem,flag = parse_page(html)</span><br><span class="line"></span><br><span class="line">		print(<span class="string">'爬取第'</span>+str(i)+<span class="string">'页'</span>)</span><br><span class="line"></span><br><span class="line">		sqlalchemy_helper.save_db(tem)</span><br><span class="line">		<span class="comment"># result.extend(tem)</span></span><br><span class="line">		<span class="comment"># save_as_json(tem)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">	main()</span><br></pre></td></tr></table></figure>
<h4 id="3-数据直接嵌入script代码里面"><a href="#3-数据直接嵌入script代码里面" class="headerlink" title="3.数据直接嵌入script代码里面"></a>3.数据直接嵌入script代码里面</h4><p>数据如果是嵌入到script里面的，那么获取数据的方式和第一种一样</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""__author__=wuliang"""</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> movies.redis_conn <span class="keyword">import</span> get_redis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page</span><span class="params">(url)</span>:</span></span><br><span class="line">	headers = &#123;</span><br><span class="line">		<span class="string">'User-Agent'</span>: <span class="string">'User-Agent:Mozilla/5.0 (Windows NT 10.0; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0'</span></span><br><span class="line">	&#125;</span><br><span class="line">	response = requests.get(url, headers=headers)</span><br><span class="line">	<span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">		<span class="keyword">return</span> response.content.decode(<span class="string">'utf8'</span>)</span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(html)</span>:</span></span><br><span class="line">	pattern = re.compile (<span class="string">'downurls="第.*?集\$(.*?)"'</span>, re.S)</span><br><span class="line">	pattern2 = re.compile(<span class="string">'value="ed2k:(.*?)"'</span>,re.S)</span><br><span class="line">	lists = re.findall(pattern, html)</span><br><span class="line">	list2 = re.findall(pattern2, html)</span><br><span class="line">	<span class="keyword">for</span> li <span class="keyword">in</span> list2:</span><br><span class="line">		li = <span class="string">'ed2k:'</span>+li</span><br><span class="line">	lists += list2</span><br><span class="line">	<span class="keyword">for</span> item <span class="keyword">in</span> lists:</span><br><span class="line">		<span class="keyword">if</span> item == <span class="string">''</span>:</span><br><span class="line">			lists.remove(item)</span><br><span class="line">	<span class="keyword">if</span> lists!= []:</span><br><span class="line">		content = json.dumps(lists, ensure_ascii=<span class="keyword">False</span>)</span><br><span class="line">		<span class="keyword">with</span> open(<span class="string">'./dianying.txt'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">			f.write(content)</span><br><span class="line">			print(<span class="string">'正在写入电影'</span>+str(len(lists))+<span class="string">'条链接'</span>)</span><br><span class="line">			print(<span class="string">'-----------------------------'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_url</span><span class="params">(link_list)</span>:</span></span><br><span class="line">	conn = get_redis()</span><br><span class="line">	<span class="keyword">if</span> len(conn.smembers(<span class="string">'url'</span>)) &lt; <span class="number">5</span>:</span><br><span class="line">		<span class="keyword">for</span> item <span class="keyword">in</span> link_list:</span><br><span class="line">			conn.sadd(<span class="string">'url'</span>,item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_url</span><span class="params">(html)</span>:</span></span><br><span class="line">	pattern = re.compile (<span class="string">'href="/(.*?)"'</span>, re.S)</span><br><span class="line">	lists = re.findall(pattern,html)</span><br><span class="line">	link_list = []</span><br><span class="line">	<span class="keyword">for</span> item <span class="keyword">in</span> lists:</span><br><span class="line">		<span class="keyword">if</span> <span class="string">'.css'</span> <span class="keyword">not</span> <span class="keyword">in</span> item <span class="keyword">and</span> item != <span class="string">''</span> <span class="keyword">and</span> <span class="string">'.ico'</span> <span class="keyword">not</span> <span class="keyword">in</span> item:</span><br><span class="line">			link_list.append(<span class="string">'/'</span>+item)</span><br><span class="line">	<span class="keyword">return</span> link_list</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_url</span><span class="params">(url)</span>:</span></span><br><span class="line">	conn = get_redis()</span><br><span class="line">	t_url = conn.spop(<span class="string">'url'</span>).decode(<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">	neweul = url + t_url</span><br><span class="line">	<span class="keyword">return</span> neweul</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">	org_url = <span class="string">'https://www.mkv99.com'</span></span><br><span class="line">	url = org_url</span><br><span class="line">	count = <span class="number">0</span></span><br><span class="line">	html = get_page (url)</span><br><span class="line">	<span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">		<span class="keyword">if</span> <span class="keyword">not</span> html:</span><br><span class="line">			html = get_page(url)</span><br><span class="line">		link_url = get_all_url (html)</span><br><span class="line">		save_url (link_url)</span><br><span class="line">		url = get_url(org_url)</span><br><span class="line">		parse_page(html)</span><br><span class="line"></span><br><span class="line">		html = <span class="string">''</span></span><br><span class="line">		count += <span class="number">1</span></span><br><span class="line">		print(<span class="string">'第'</span>+str(count)+<span class="string">'次爬取'</span>)</span><br><span class="line">			</span><br><span class="line">		<span class="keyword">if</span> count == <span class="number">100</span>:</span><br><span class="line">			name = input(<span class="string">'是否结束爬虫y|n'</span>)</span><br><span class="line">			<span class="keyword">if</span> name == <span class="string">'y'</span>:</span><br><span class="line">				<span class="keyword">break</span></span><br><span class="line">			<span class="keyword">else</span>:</span><br><span class="line">				<span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">	main()</span><br></pre></td></tr></table></figure>
<h4 id="4-数据在网页上，但是经过加密"><a href="#4-数据在网页上，但是经过加密" class="headerlink" title="4.数据在网页上，但是经过加密"></a>4.数据在网页上，但是经过加密</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""__author__=wuliang"""</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> xia_mi_music.kaisha <span class="keyword">import</span> kaisha_decode</span><br><span class="line"></span><br><span class="line"><span class="string">"""__author__=wuliang"""</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model.sqlalchemy_helper <span class="keyword">import</span> save_db</span><br><span class="line"></span><br><span class="line">chrome_options = webdriver.ChromeOptions()</span><br><span class="line">browser = webdriver.Chrome(chrome_options=chrome_options)</span><br><span class="line"></span><br><span class="line">browser.set_window_size(<span class="number">1400</span>, <span class="number">700</span>)</span><br><span class="line">wait = WebDriverWait(browser, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page</span><span class="params">()</span>:</span></span><br><span class="line">	url = <span class="string">'https://www.xiami.com/chart?spm=a1z1s.6843761.1110925385.2.9FhuN1'</span></span><br><span class="line">	browser.get(url)</span><br><span class="line">	time.sleep(<span class="number">3</span>)</span><br><span class="line">	<span class="keyword">return</span> browser.page_source</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(page_source)</span>:</span></span><br><span class="line">	html = etree.HTML(page_source)</span><br><span class="line">	song_div = html.xpath(<span class="string">'//div[@id="chart"]'</span>)</span><br><span class="line">	song_url  = song_div[<span class="number">0</span>].xpath(<span class="string">'./table/tr[@class="songwrapper"]/@data-mp3'</span>)</span><br><span class="line">	song_name = song_div[<span class="number">0</span>].xpath(<span class="string">'./table/tr[@class="songwrapper"]/@data-title'</span>)</span><br><span class="line">	song_url = kaisha_decode(song_url)</span><br><span class="line"></span><br><span class="line">	result_list = []</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(len(song_url)):</span><br><span class="line">		song_dict = &#123;</span><br><span class="line">			<span class="string">'mp3'</span>: song_url[i],</span><br><span class="line">			<span class="string">'name'</span>: song_name[i]</span><br><span class="line">		&#125;</span><br><span class="line">		result_list.append(song_dict)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> result_list</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mp3</span><span class="params">(url)</span>:</span></span><br><span class="line">	headers = &#123;</span><br><span class="line">		<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60'</span></span><br><span class="line">	&#125;</span><br><span class="line">	response = requests.get(url, headers=headers)</span><br><span class="line">	<span class="keyword">return</span> response.content</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_mp3</span><span class="params">(result)</span>:</span></span><br><span class="line">	<span class="keyword">for</span> item <span class="keyword">in</span> result:</span><br><span class="line">		url = item.get(<span class="string">'mp3'</span>)</span><br><span class="line">		mp3_data = get_mp3(url)</span><br><span class="line">		name = item.get(<span class="string">'name'</span>)</span><br><span class="line">		<span class="keyword">if</span> <span class="string">'24'</span> <span class="keyword">in</span> name:</span><br><span class="line">			name= <span class="string">'24-7'</span></span><br><span class="line">		<span class="keyword">try</span>:</span><br><span class="line">			<span class="keyword">with</span> open(<span class="string">'./mp3/'</span>+name+<span class="string">'.mp3'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">				f.write(mp3_data)</span><br><span class="line">		<span class="keyword">except</span>:</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">	page_source = get_page()</span><br><span class="line">	result = parse_page(page_source)</span><br><span class="line">	save_mp3(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">	main()</span><br></pre></td></tr></table></figure>

        </section>
    </article>
    
        <!-- disqus 评论框 start -->
        <div class="comment">
            <div id="disqus_thread" class="disqus-thread">
              <i>Loading comments box needs to over the wall</i>
            </div>
        </div>
        <!-- disqus 评论框 end -->
    
    
        <!-- livere 评论框 start -->
        <div class="comment">
            <div id="lv-container" data-id="city" data-uid="your_livere_uid"></div>
        </div>
        <!-- livere 评论框 end -->
        
  </div>
  <aside>
    
    <div class="toc-container">
        <h1>目录</h1>
        <div class="content">
            <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#目前遇到的数据加载方式"><span class="toc-number">1.</span> <span class="toc-text">目前遇到的数据加载方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-数据直接在网页上"><span class="toc-number">1.1.</span> <span class="toc-text">1.数据直接在网页上</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-数据通过异步加载的方式返回的"><span class="toc-number">1.2.</span> <span class="toc-text">2.数据通过异步加载的方式返回的</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-数据直接嵌入script代码里面"><span class="toc-number">1.3.</span> <span class="toc-text">3.数据直接嵌入script代码里面</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-数据在网页上，但是经过加密"><span class="toc-number">1.4.</span> <span class="toc-text">4.数据在网页上，但是经过加密</span></a></li></ol></li></ol>
        </div>
    </div>
    
  </aside>
</main>

<!-- disqus 公共JS代码 -->
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES * * */
  var disqus_shortname = "your_disqus_shortname";
  var disqus_identifier = "http://yoursite.com/2018/12/01/爬虫小总结/";
  var disqus_url = "http://yoursite.com/2018/12/01/爬虫小总结/";

  isAgent(getDisqus)

  // determine user agent in China
  function isAgent(cb) {
    var url = '//graph.facebook.com/feed?callback=h';
    var xhr = new XMLHttpRequest();
    var called = false;
    xhr.open('GET', url);
    xhr.onreadystatechange = function() {
      if (xhr.readyState === 4 && xhr.status === 200) {
      called = true;
      cb(true);
      }
    };
    xhr.send();
    // timeout 1s, this facebook API is very fast.
    setTimeout(function() {
      if (!called) {
      xhr.abort();
      cb(false)
      }
    }, 1000);
  }

  function getDisqus(isAgent) {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; 
    dsq.async = true
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq)
  }
</script>
<!-- disqus 公共JS代码 end -->


<script type="text/javascript">
  (function(d, s) {
      var j, e = d.getElementsByTagName(s)[0];

      if (typeof LivereTower === 'function') { return; }

      j = d.createElement(s);
      j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
      j.async = true;

      e.parentNode.insertBefore(j, e);
  })(document, 'script');
</script>


  <footer>
  <div class="copyright">
    <div>
      &copy; 2018 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a>&nbsp
    </div>
    <div>
      Theme by <a href="https://github.com/lewis-geek/hexo-theme-Aath" target="_blank">Aath</a>
    </div>
  </div>
</footer>


<script src="https://cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<script src="/lib/in-view.min.js"></script>
<script src="/lib/lodash.min.js"></script>
<script>
  var isDown = true
  var oldY = 0
  inView.offset(50)

  document.body.addEventListener('touchstart', function(){});
  
  window.addEventListener('scroll', _.throttle(e => {
    var currentY = window.scrollY
    if((oldY - currentY) < 0) {
      isDown = true
    } else {
      isDown = false
    }
    oldY = currentY
  }, 250))

  $("article img").each(function() {
      var strA = "<a data-fancybox='gallery' href='" + this.src + "'></a>";
      $(this).wrapAll(strA);
  });

  $('.toc-link').each(function() {
      var href = $(this).attr("href");
      
      inView(href).on('exit', () => {
        if (isDown) {
          handleActive(href)
        }
      })

      inView(href).on('enter', () => {
        if (!isDown) {
          handleActive(href)
        }
      })

      this.onclick = function(e) {
        var pos = $(href).offset().top - 10;
        $("html,body").animate({scrollTop: pos}, 300);
        setTimeout(() => {
          handleActive(href)
        }, 350)
        return false
      }
  })

  function handleActive(href) {
    document.querySelectorAll('.toc-link').forEach(elm => {
      elm.classList.remove('active')
    })
    document.querySelector(".toc [href='"+ href +"']").classList.add('active')
  }
</script>
<script src="/lib/jquery.fancybox.min.js"></script>


</body>
</html>
